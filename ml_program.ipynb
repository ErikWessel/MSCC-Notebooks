{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =   ['CLR', 'FEW', 'SCT', 'BKN', 'OVC']\n",
    "classes = list(range(len(class_names)))\n",
    "class_dict = {name: i for i, name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = EfficientNet_V2_S_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erikw\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = efficientnet_v2_s(weights)\n",
    "\n",
    "for param in model.parameters(): #freeze model\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[1] = nn.Linear(1280, len(classes))\n",
    "model = model.to(device)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [16, 3, 300, 300]    [16, 5]              --                   Partial\n",
       "├─Sequential (features)                                      [16, 3, 300, 300]    [16, 1280, 10, 10]   --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [16, 3, 300, 300]    [16, 24, 150, 150]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [16, 3, 300, 300]    [16, 24, 150, 150]   (648)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [16, 24, 150, 150]   [16, 24, 150, 150]   (48)                 False\n",
       "│    │    └─SiLU (2)                                         [16, 24, 150, 150]   [16, 24, 150, 150]   --                   --\n",
       "│    └─Sequential (1)                                        [16, 24, 150, 150]   [16, 24, 150, 150]   --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [16, 24, 150, 150]   [16, 24, 150, 150]   (5,232)              False\n",
       "│    │    └─FusedMBConv (1)                                  [16, 24, 150, 150]   [16, 24, 150, 150]   (5,232)              False\n",
       "│    └─Sequential (2)                                        [16, 24, 150, 150]   [16, 48, 75, 75]     --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [16, 24, 150, 150]   [16, 48, 75, 75]     (25,632)             False\n",
       "│    │    └─FusedMBConv (1)                                  [16, 48, 75, 75]     [16, 48, 75, 75]     (92,640)             False\n",
       "│    │    └─FusedMBConv (2)                                  [16, 48, 75, 75]     [16, 48, 75, 75]     (92,640)             False\n",
       "│    │    └─FusedMBConv (3)                                  [16, 48, 75, 75]     [16, 48, 75, 75]     (92,640)             False\n",
       "│    └─Sequential (3)                                        [16, 48, 75, 75]     [16, 64, 38, 38]     --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [16, 48, 75, 75]     [16, 64, 38, 38]     (95,744)             False\n",
       "│    │    └─FusedMBConv (1)                                  [16, 64, 38, 38]     [16, 64, 38, 38]     (164,480)            False\n",
       "│    │    └─FusedMBConv (2)                                  [16, 64, 38, 38]     [16, 64, 38, 38]     (164,480)            False\n",
       "│    │    └─FusedMBConv (3)                                  [16, 64, 38, 38]     [16, 64, 38, 38]     (164,480)            False\n",
       "│    └─Sequential (4)                                        [16, 64, 38, 38]     [16, 128, 19, 19]    --                   False\n",
       "│    │    └─MBConv (0)                                       [16, 64, 38, 38]     [16, 128, 19, 19]    (61,200)             False\n",
       "│    │    └─MBConv (1)                                       [16, 128, 19, 19]    [16, 128, 19, 19]    (171,296)            False\n",
       "│    │    └─MBConv (2)                                       [16, 128, 19, 19]    [16, 128, 19, 19]    (171,296)            False\n",
       "│    │    └─MBConv (3)                                       [16, 128, 19, 19]    [16, 128, 19, 19]    (171,296)            False\n",
       "│    │    └─MBConv (4)                                       [16, 128, 19, 19]    [16, 128, 19, 19]    (171,296)            False\n",
       "│    │    └─MBConv (5)                                       [16, 128, 19, 19]    [16, 128, 19, 19]    (171,296)            False\n",
       "│    └─Sequential (5)                                        [16, 128, 19, 19]    [16, 160, 19, 19]    --                   False\n",
       "│    │    └─MBConv (0)                                       [16, 128, 19, 19]    [16, 160, 19, 19]    (281,440)            False\n",
       "│    │    └─MBConv (1)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (2)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (3)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (4)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (5)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (6)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (7)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    │    └─MBConv (8)                                       [16, 160, 19, 19]    [16, 160, 19, 19]    (397,800)            False\n",
       "│    └─Sequential (6)                                        [16, 160, 19, 19]    [16, 256, 10, 10]    --                   False\n",
       "│    │    └─MBConv (0)                                       [16, 160, 19, 19]    [16, 256, 10, 10]    (490,152)            False\n",
       "│    │    └─MBConv (1)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (2)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (3)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (4)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (5)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (6)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (7)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (8)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (9)                                       [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (10)                                      [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (11)                                      [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (12)                                      [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (13)                                      [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    │    └─MBConv (14)                                      [16, 256, 10, 10]    [16, 256, 10, 10]    (1,005,120)          False\n",
       "│    └─Conv2dNormActivation (7)                              [16, 256, 10, 10]    [16, 1280, 10, 10]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [16, 256, 10, 10]    [16, 1280, 10, 10]   (327,680)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [16, 1280, 10, 10]   [16, 1280, 10, 10]   (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [16, 1280, 10, 10]   [16, 1280, 10, 10]   --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [16, 1280, 10, 10]   [16, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [16, 1280]           [16, 5]              --                   True\n",
       "│    └─Dropout (0)                                           [16, 1280]           [16, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [16, 1280]           [16, 5]              6,405                True\n",
       "============================================================================================================================================\n",
       "Total params: 20,183,893\n",
       "Trainable params: 6,405\n",
       "Non-trainable params: 20,177,488\n",
       "Total mult-adds (G): 84.90\n",
       "============================================================================================================================================\n",
       "Input size (MB): 17.28\n",
       "Forward/backward pass size (MB): 5815.25\n",
       "Params size (MB): 80.74\n",
       "Estimated Total Size (MB): 5913.26\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    model,\n",
    "    input_size=(batch_size, 3, 300, 300),\n",
    "    verbose=0,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "dataset_dir = os.path.join(data_dir, 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(str, Enum):\n",
    "    TRAINING = 'training'\n",
    "    VALIDATION = 'validation'\n",
    "    TEST = 'test'\n",
    "\n",
    "class AimlsseImageDataset(Dataset):\n",
    "    def __init__(self, type:DatasetType, dataset_dir:str, bands:List[str], station_radius:float,\n",
    "                 transfrom=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.bands_metadata = pd.read_csv(os.path.join(dataset_dir, f'bands_metadata.csv'))\n",
    "        self.bands_metadata.set_index('band', inplace=True)\n",
    "        bands_series = pd.Series(bands)\n",
    "        invalid_bands = bands_series[~bands_series.isin(self.bands_metadata.index)].to_list()\n",
    "        if any(invalid_bands):\n",
    "            raise ValueError(f'Bands {invalid_bands} are not valid')\n",
    "        if len(self.bands_metadata.loc[bands]['resolution [M]'].unique()) != 1:\n",
    "            raise ValueError(f'Bands {bands} do not have the same resolution')\n",
    "        self.bands = bands\n",
    "        self.station_radius = station_radius\n",
    "        self.img_labels = pd.read_csv(os.path.join(dataset_dir, f'{type.value}_labels.csv'))\n",
    "        self.img_dir = os.path.join(dataset_dir, type.value)\n",
    "        self.transform = transfrom\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[np.ndarray, str]:\n",
    "        img_paths = [os.path.join(self.img_dir, self.img_labels.iloc[index]['station'],\n",
    "                                  self.img_labels.iloc[index]['product_id'], f'{band}.jp2') for band in self.bands]\n",
    "        images = [rasterio.open(path) for path in img_paths]\n",
    "        image_tensors: List[np.ndarray] = [img.read(1).astype(np.float32) / 10000 for img in images]\n",
    "        [img.close() for img in images]\n",
    "        # Resize images with small size-differences to make them stackable\n",
    "        image_side_length = int(1 + math.floor(2.0 * self.station_radius / float(self.bands_metadata.loc[self.bands].iloc[0]['resolution [M]'])))\n",
    "        new_shape = (image_side_length, image_side_length)\n",
    "        [img.resize(new_shape, refcheck=False) for img in image_tensors]\n",
    "        # Stack images into final image, select label and perform transformations\n",
    "        image_tensors_stacked = np.stack(image_tensors, axis=2)\n",
    "        label = class_dict[self.img_labels.iloc[index]['max cloud cover']]\n",
    "        if self.transform:\n",
    "            image_tensors_stacked = self.transform(image_tensors_stacked)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image_tensors_stacked, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((300, 300), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['B4', 'B3', 'B2']\n",
    "station_radius = 8000.0\n",
    "train_dataset = AimlsseImageDataset(DatasetType.TRAINING,           dataset_dir, bands, station_radius, transfrom=composed_transforms)\n",
    "validation_dataset = AimlsseImageDataset(DatasetType.VALIDATION,    dataset_dir, bands, station_radius, transfrom=composed_transforms)\n",
    "test_dataset = AimlsseImageDataset(DatasetType.TEST,                dataset_dir, bands, station_radius, transfrom=composed_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =      DataLoader(train_dataset,       batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset,  batch_size=batch_size, shuffle=True)\n",
    "test_dataloader =       DataLoader(test_dataset,        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples = False\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def imshow(inp: torch.Tensor, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.axis('off')\n",
    "    ax.imshow(inp.permute(1, 2, 0))\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "if show_samples:\n",
    "    # Get a batch of training data\n",
    "    inputs, classes = next(iter(train_dataloader))\n",
    "    print(inputs.shape, [class_names[c] for c in classes.unique().numpy()])\n",
    "\n",
    "    # Make a grid from batch\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "    out.shape\n",
    "    \n",
    "    imshow(out)\n",
    "\n",
    "    for row in list(chunks([class_names[c] for c in classes.numpy()], 8)):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataset = train_dataset\n",
    "                dataloader = train_dataloader\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                dataset = validation_dataset\n",
    "                dataloader = validation_dataloader\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(validation_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {classes[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.9729 Acc: 0.6928\n",
      "val Loss: 1.0201 Acc: 0.6650\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.9575 Acc: 0.7134\n",
      "val Loss: 1.1573 Acc: 0.6751\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.8770 Acc: 0.7339\n",
      "val Loss: 1.2605 Acc: 0.6464\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.8908 Acc: 0.7366\n",
      "val Loss: 1.2676 Acc: 0.6447\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_trained \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, scheduler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[33], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     26\u001b[0m running_corrects \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[39m# Iterate over data.\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     30\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mAimlsseImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m     29\u001b[0m     img_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[index][\u001b[39m'\u001b[39m\u001b[39mstation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     30\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[index][\u001b[39m'\u001b[39m\u001b[39mproduct_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mband\u001b[39m}\u001b[39;00m\u001b[39m.jp2\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m band \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbands]\n\u001b[1;32m---> 31\u001b[0m     images \u001b[39m=\u001b[39m [rasterio\u001b[39m.\u001b[39mopen(path) \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m img_paths]\n\u001b[0;32m     32\u001b[0m     image_tensors: List[np\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m [img\u001b[39m.\u001b[39mread(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m10000\u001b[39m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     33\u001b[0m     [img\u001b[39m.\u001b[39mclose() \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images]\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m     29\u001b[0m     img_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[index][\u001b[39m'\u001b[39m\u001b[39mstation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     30\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[index][\u001b[39m'\u001b[39m\u001b[39mproduct_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mband\u001b[39m}\u001b[39;00m\u001b[39m.jp2\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m band \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbands]\n\u001b[1;32m---> 31\u001b[0m     images \u001b[39m=\u001b[39m [rasterio\u001b[39m.\u001b[39;49mopen(path) \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m img_paths]\n\u001b[0;32m     32\u001b[0m     image_tensors: List[np\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m [img\u001b[39m.\u001b[39mread(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m10000\u001b[39m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     33\u001b[0m     [img\u001b[39m.\u001b[39mclose() \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\rasterio\\env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     session \u001b[39m=\u001b[39m DummySession()\n\u001b[0;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m env_ctor(session\u001b[39m=\u001b[39msession):\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\rasterio\\__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m path \u001b[39m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 304\u001b[0m     dataset \u001b[39m=\u001b[39m DatasetReader(path, driver\u001b[39m=\u001b[39mdriver, sharing\u001b[39m=\u001b[39msharing, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    305\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    306\u001b[0m     dataset \u001b[39m=\u001b[39m get_writer_for_path(path, driver\u001b[39m=\u001b[39mdriver)(\n\u001b[0;32m    307\u001b[0m         path, mode, driver\u001b[39m=\u001b[39mdriver, sharing\u001b[39m=\u001b[39msharing, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32mrasterio\\_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\erikw\\anaconda3\\lib\\site-packages\\rasterio\\_path.py:113\u001b[0m, in \u001b[0;36m_UnparsedPath.name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Encapsulates legacy GDAL filenames\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mAttributes\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m    The legacy GDAL filename.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m path \u001b[39m=\u001b[39m attr\u001b[39m.\u001b[39mib()\n\u001b[1;32m--> 113\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mname\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The unparsed path's original path\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, scheduler, num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d581a96df5f4d85c539a287c4f6ef29fb4dda2cc3374c000ae58cf3e6d0b188e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
