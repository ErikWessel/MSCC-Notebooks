{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "from ml_commons import *\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import (EfficientNet_V2_S_Weights, Swin_V2_S_Weights,\n",
    "                                efficientnet_v2_s, swin_v2_s)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('ml_config.yml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix_dir = config['paths']['prefix_dir']\n",
    "dataset_dir = os.path.join(prefix_dir, config['paths']['dataset_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = os.path.join(config['paths']['machine_learning_dir'], 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model:nn.Module, name:str):\n",
    "    spacing = '  '\n",
    "    model_str = spacing + f'\\n{spacing}'.join(str(model).splitlines())\n",
    "    print(f'--- {name} ---\\n{model_str}\\n{\"-\" * (8 + len(name))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: EfficientNet\n",
      "--- Initial Model Head ---\n",
      "  Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "--------------------------\n",
      "Classification layer has 1280 input features\n",
      "--- Modified Model Head ---\n",
      "  Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'Using model: ' + config['model']['name'])\n",
    "model_name = str(config['model']['name']).lower()\n",
    "if model_name == 'swintransformer':\n",
    "    model = swin_v2_s(weights=Swin_V2_S_Weights.DEFAULT)\n",
    "elif model_name == 'efficientnet':\n",
    "    model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "else:\n",
    "    raise RuntimeError(f'Model \"' + config['model']['name'] + '\" is unknown')\n",
    "\n",
    "if config['model']['freeze_parameters']:\n",
    "    for param in model.parameters(): #freeze model\n",
    "        param.requires_grad = False\n",
    "\n",
    "print_model(get_model_head(model), 'Initial Model Head')\n",
    "num_features = get_num_features(model)\n",
    "print(f'Classification layer has {num_features} input features')\n",
    "new_model_head = nn.Sequential()\n",
    "# Before linear layer\n",
    "if config['processing']['use_dropout']:\n",
    "    new_model_head.append(nn.Dropout(p=config['processing']['dropout_p'], inplace=True))\n",
    "# Linear layer\n",
    "new_model_head.append(nn.Linear(num_features, len(classes)))\n",
    "# After linear layer\n",
    "if config['processing']['use_ordinal_regression'] and config['processing']['activation_function'] != False:\n",
    "    activation_function = str(config['processing']['activation_function']).lower()\n",
    "    if activation_function == 'sigmoid':\n",
    "        activation_function = nn.Sigmoid()\n",
    "    elif activation_function == 'relu':\n",
    "        activation_function = nn.ReLU()\n",
    "    elif activation_function == 'tanh':\n",
    "        activation_function = nn.Tanh()\n",
    "    else:\n",
    "        raise RuntimeError(f'Unkown activation function: {activation_function}')\n",
    "    new_model_head = new_model_head.append(activation_function)\n",
    "\n",
    "set_model_head(model, new_model_head)\n",
    "print_model(get_model_head(model), 'Modified Model Head')\n",
    "# model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>CLR</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SCT</th>\n",
       "      <th>BKN</th>\n",
       "      <th>OVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>1.822467</td>\n",
       "      <td>14.258716</td>\n",
       "      <td>18.502381</td>\n",
       "      <td>17.988426</td>\n",
       "      <td>3.682938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label        CLR        FEW        SCT        BKN       OVC\n",
       "weight  1.822467  14.258716  18.502381  17.988426  3.682938"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_weights = pd.read_csv(os.path.join(dataset_dir, 'training_weights.csv'), index_col='label')\n",
    "training_weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_label(pred: torch.Tensor) -> torch.Tensor:\n",
    "    return (pred > 0.5).cumprod(axis=1).sum(axis=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalRegression():\n",
    "    def __init__(self, weights:Optional[torch.Tensor]) -> None:\n",
    "        if weights is None:\n",
    "            self.weights = torch.Tensor([1] * len(classes))\n",
    "            self.weights.to(device)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "    def __call__(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        modified_targets = torch.zeros_like(predictions)\n",
    "        for i, target in enumerate(targets):\n",
    "            modified_targets[i, 0 : target + 1] = 1\n",
    "        return torch.mean((nn.MSELoss(reduction='none')(predictions, modified_targets) * self.weights).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(loader:DataLoader):\n",
    "  sum, squared_sum, num_batches = 0,0,0\n",
    "  for data, _, _ in loader:\n",
    "    sum += torch.mean(data,dim=[0,1,2])\n",
    "    squared_sum += torch.mean(data**2,dim=[0,1,2])\n",
    "    num_batches += 1\n",
    "  mean = sum/num_batches\n",
    "  std = (squared_sum/num_batches - mean**2)**0.5\n",
    "  return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_transforms = [\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "]\n",
    "composed_transforms = transforms.Compose(dataset_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_manual_labels = config['processing']['use_manual_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'use_manual_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset       \u001b[39m=\u001b[39m AimlsseImageDataset(DatasetType\u001b[39m.\u001b[39;49mTRAINING,     dataset_dir, transfrom\u001b[39m=\u001b[39;49mcomposed_transforms, use_manual_labels\u001b[39m=\u001b[39;49muse_manual_labels)\n\u001b[0;32m      2\u001b[0m validation_dataset  \u001b[39m=\u001b[39m AimlsseImageDataset(DatasetType\u001b[39m.\u001b[39mVALIDATION,   dataset_dir, transfrom\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, use_manual_labels\u001b[39m=\u001b[39muse_manual_labels)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'use_manual_labels'"
     ]
    }
   ],
   "source": [
    "train_dataset       = AimlsseImageDataset(DatasetType.TRAINING,     dataset_dir, transfrom=composed_transforms, use_manual_labels=use_manual_labels)\n",
    "validation_dataset  = AimlsseImageDataset(DatasetType.VALIDATION,   dataset_dir, transfrom=None, use_manual_labels=use_manual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(dataset:AimlsseImageDataset, dataset_type:DatasetType, dataset_transforms):\n",
    "    mean, std = mean_std(dataset)\n",
    "    print(f'{dataset_type.name} - mean {mean:.3f}, std {std:.3f}')\n",
    "    if dataset_transforms is None:\n",
    "        dataset_transforms = []\n",
    "    return AimlsseImageDataset(dataset_type, dataset_dir,\n",
    "                               transfrom = transforms.Compose(dataset_transforms + [transforms.Normalize(mean, std)]),\n",
    "                               use_manual_labels=use_manual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['processing']['batch_normalization']:\n",
    "    train_dataset       = batch_normalization(train_dataset, DatasetType.TRAINING, dataset_transforms)\n",
    "    validation_dataset  = batch_normalization(validation_dataset, DatasetType.VALIDATION, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['processing']['use_weighted_sampler']:\n",
    "    num_samples = len(train_dataset)\n",
    "    weights = [0] * num_samples\n",
    "    for i in range(num_samples):\n",
    "        label = train_dataset.get_label(i)\n",
    "        weights[i] = training_weights.loc[class_names]['weight'].iloc[label]\n",
    "    training_sampler = WeightedRandomSampler(weights, num_samples)\n",
    "    train_dataloader =  DataLoader(train_dataset,       batch_size=config['processing']['batch_size'], sampler=training_sampler)\n",
    "else:\n",
    "    train_dataloader =  DataLoader(train_dataset,       batch_size=config['processing']['batch_size'], shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset,  batch_size=config['processing']['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['output']['show_samples']:\n",
    "    plot_samples(train_dataset, config['processing']['batch_size'], sample_batch_index)\n",
    "    sample_batch_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss weights: tensor([ 1.8225, 14.2587, 18.5024, 17.9884,  3.6829], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if config['processing']['use_weighted_loss_function']:\n",
    "    loss_function_weights = torch.tensor(training_weights['weight'].to_list())\n",
    "    loss_function_weights = loss_function_weights.to(device)\n",
    "else:\n",
    "    loss_function_weights = None\n",
    "print(f'Loss weights: {loss_function_weights}')\n",
    "\n",
    "if config['processing']['use_ordinal_regression']:\n",
    "    criterion = OrdinalRegression(loss_function_weights)\n",
    "    outputs_to_predictions = prediction_to_label\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(loss_function_weights)\n",
    "    outputs_to_predictions = lambda outputs: torch.max(outputs, 1)[1]\n",
    "\n",
    "learning_rate = math.pow(10, -config['processing']['learning_rate_exp'])\n",
    "weight_decay = math.pow(10, -config['processing']['weight_decay_exp']) if config['processing']['use_weight_decay'] else 0.0\n",
    "optimizer_name = str(config['processing']['optimizer']).lower()\n",
    "if optimizer_name == 'adam':\n",
    "    optimizer = optim.Adam(get_model_head(model).parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "elif optimizer_name == 'sgd':\n",
    "    optimizer = optim.SGD(get_model_head(model).parameters(), lr=learning_rate, momentum=config['processing']['momentum'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Checkpoints will be stored in: ML\\checkpoints\\chk.pt\n",
      "The results will be stored in: ML\\output\\local_dummy.pt\n",
      "Epoch 5/31\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/486 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m output_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(config[\u001b[39m'\u001b[39m\u001b[39mpaths\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmachine_learning_dir\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m, config[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39moutput_name\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe results will be stored in: \u001b[39m\u001b[39m{\u001b[39;00moutput_filepath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model_trained \u001b[39m=\u001b[39m train_model(model, device, model_data, criterion, outputs_to_predictions, optimizer, scheduler,\n\u001b[0;32m      7\u001b[0m                             checkpoint_filepath, num_epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mprocessing\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mnum_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      8\u001b[0m                             batch_accumulation\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mprocessing\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mbatch_accumulation\u001b[39;49m\u001b[39m'\u001b[39;49m], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCopying data from checkpoint to results..\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m state \u001b[39m=\u001b[39m load_state(checkpoint_filepath)\n",
      "File \u001b[1;32mc:\\Users\\erikw\\Documents\\Uni\\Master\\Semester 5\\Master Thesis\\python\\aimlsse\\notebooks\\ml_commons.py:294\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, device, data, criterion, outputs_to_predictions, optimizer, scheduler, checkpoint_filepath, num_epochs, batch_accumulation, load_checkpoint, config)\u001b[0m\n\u001b[0;32m    291\u001b[0m             optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    293\u001b[0m \u001b[39m# statistics\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m    295\u001b[0m running_corrects \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m==\u001b[39m labels\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    296\u001b[0m y_true\u001b[39m.\u001b[39mextend(labels\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_data = ModelData(train_dataset, validation_dataset, train_dataloader, validation_dataloader)\n",
    "checkpoint_filepath = os.path.join(config['paths']['machine_learning_dir'], 'checkpoints', 'chk.pt')\n",
    "print(f'Model Checkpoints will be stored in: {checkpoint_filepath}')\n",
    "output_filepath = os.path.join(config['paths']['machine_learning_dir'], 'output', config['output']['output_name'])\n",
    "print(f'The results will be stored in: {output_filepath}')\n",
    "model_trained = train_model(model, device, model_data, criterion, outputs_to_predictions, optimizer, scheduler,\n",
    "                            checkpoint_filepath, num_epochs=config['processing']['num_epochs'],\n",
    "                            batch_accumulation=config['processing']['batch_accumulation'], config=config)\n",
    "print('Copying data from checkpoint to results..')\n",
    "state = load_state(checkpoint_filepath)\n",
    "save_state(output_filepath, state)\n",
    "print(f'Results stored in: {output_filepath}')\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d581a96df5f4d85c539a287c4f6ef29fb4dda2cc3374c000ae58cf3e6d0b188e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
